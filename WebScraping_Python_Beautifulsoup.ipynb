{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item(from_page, to_page, base_url='https://www.desocialekaart.be/zoek?page={}'):\n",
    "    \n",
    "    \"\"\" \n",
    "        The function to get the URLs from the target website in order to scrape. \n",
    "  \n",
    "        Parameters: \n",
    "            base_url : The main part of the https link of the website that will be scraped.\n",
    "            from_page : The starting range parameter to get the specific link if the website has many pages or sub_pages\n",
    "            to_page : The end_range parameter to get the specific link if the website has many pages or sub_pages\n",
    "          \n",
    "        Returns: \n",
    "            links: A list of URLs \n",
    "    \"\"\"\n",
    "\n",
    "    links = []\n",
    "    for page_number in range(from_page,to_page):\n",
    "        page_is_not_processed = True\n",
    "        while page_is_not_processed:\n",
    "\n",
    "            try:\n",
    "                page = requests.get(base_url.format(page_number)+\"&where=\")\n",
    "                soup = BeautifulSoup(page.text, 'html.parser')\n",
    "                name_links = soup.find(id='search-results')\n",
    "                links_list = name_links.find_all(\"a\")\n",
    "                for element in links_list:\n",
    "                    links.append('https://www.desocialekaart.be' + element.get('href'))\n",
    "                \n",
    "                page_is_not_processed = False\n",
    "\n",
    "            except:\n",
    "                print(\"I got an error while parsing page {}.. I'll retry soon.\".format(page_number))\n",
    "                time.sleep(0.5)\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngo_info(ngo_page_url):\n",
    "    \n",
    "    \"\"\" \n",
    "        The function to get information in text format from the target website (a single page) scraping by Beatifulsoap object.\n",
    "        \n",
    "  \n",
    "        Parameters: \n",
    "            ngo_page_url : An URL\n",
    "          \n",
    "        Returns: \n",
    "            results: A dictionary of required data(each key of this dictionary is to be a column)\n",
    "                    and source link(to catch the errors) \n",
    "    \"\"\"\n",
    "    \n",
    "    #page_is_not_processed = True\n",
    "    #while page_is_not_processed:\n",
    "        \n",
    "    try:\n",
    "        results={}\n",
    "        results[\"source_page\"] = ngo_page_url\n",
    "\n",
    "        res = requests.get(ngo_page_url)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        if str(res.status_code) == \"200\" : \n",
    "\n",
    "            try:\n",
    "                temp_infos = [i.text for i in soup.find(class_='fiche-public-name')]\n",
    "                results[\"partner_name\"] = temp_infos[0]\n",
    "            except:\n",
    "                results[\"partner_name\"] =\"null\"\n",
    "\n",
    "            try:\n",
    "                temp_infos = [i.text for i in soup.find_all('div', {'class':['street', 'number', 'postcode-city']})]\n",
    "\n",
    "                final_adress = \"\"\n",
    "                for element in temp_infos:\n",
    "                    final_adress += \" \" + element\n",
    "\n",
    "                results[\"adress\"] = final_adress\n",
    "            except:\n",
    "                results[\"adress\"] = \"null\"\n",
    "\n",
    "            try:\n",
    "                temp_infos = [i.text.strip() for i in soup.find_all(class_='field-collection-view clearfix view-mode-full field-collection-view-final')]\n",
    "                results[\"phone\"] = temp_infos[0]\n",
    "            except:\n",
    "                results[\"phone\"] = \"null\"\n",
    "\n",
    "            try:\n",
    "                temp_infos = [i for i in soup.find(class_=\"fiche-online\").find_next('a')]\n",
    "                results[\"web_adress\"] = temp_infos[0]\n",
    "            except:\n",
    "                results[\"web_adress\"] = \"null\"\n",
    "\n",
    "            try:\n",
    "                temp_infos = [i for i in soup.find(class_='fiche-email').find_next('a')]\n",
    "                results[\"email_adress\"] = temp_infos[0] \n",
    "            except:\n",
    "                results[\"email_adress\"] = \"null\"  \n",
    "\n",
    "            try:\n",
    "                temp_infos = [i.text for i in soup.find_all(class_='fiche-working')]\n",
    "                results[\"doelgroup\"] = temp_infos[0] \n",
    "            except:\n",
    "                 results[\"doelgroup\"] = \"null\"\n",
    "\n",
    "        else :\n",
    "\n",
    "            results[\"partner_name\"] =\"error\"\n",
    "            results[\"adress\"] = \"error\"\n",
    "            results[\"phone\"] = \"error\"\n",
    "            results[\"web_adress\"] = \"error\"\n",
    "            results[\"email_adress\"] = \"error\" \n",
    "            results[\"doelgroup\"] = \"error\"\n",
    "        \n",
    "    except:\n",
    "            print(\"I got an error...\")\n",
    "\n",
    "\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngos_dataframe(ngo_page_urls):\n",
    "    \n",
    "    \"\"\" \n",
    "        The function to convert scraped data into a pandas DataFrame after looping for all pages and cleansing the captured data. \n",
    "  \n",
    "        Parameters: \n",
    "            ngo_page_urls : A list of URLs \n",
    "          \n",
    "        Returns: \n",
    "            df: A data frame containing features that has been asked for. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        partner_name=[]\n",
    "        adress=[]\n",
    "        phone=[]\n",
    "        web_adress=[]\n",
    "        email_adress=[]\n",
    "        doelgroup=[]\n",
    "        source_pages=[]\n",
    "\n",
    "        for ngo_url in ngo_page_urls:\n",
    "\n",
    "            infos = get_ngo_info(ngo_url)\n",
    "            partner_name.append(infos[\"partner_name\"])\n",
    "            adress.append(infos[\"adress\"])\n",
    "            phone.append(infos[\"phone\"])\n",
    "            web_adress.append(infos[\"web_adress\"])\n",
    "            email_adress.append(infos[\"email_adress\"])\n",
    "            doelgroup.append(infos[\"doelgroup\"])\n",
    "            source_pages.append(infos[\"source_page\"])\n",
    "    except:\n",
    "        print('error..')\n",
    "        \n",
    "        df = pd.DataFrame({\"source_page\" : source_pages,\n",
    "                           \"partner_name\" : partner_name,\n",
    "                           \"adress\" : adress,\n",
    "                           \"phone\" : phone,\n",
    "                           \"web_adress\" : web_adress,\n",
    "                           \"email_adress\" : email_adress,\n",
    "                           \"doelgroup\": doelgroup})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_urls_to_scrape = get_item(from_page=1200, to_page=1300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I got an error...\n",
      "error..\n"
     ]
    }
   ],
   "source": [
    "df = get_ngos_dataframe(companies_urls_to_scrape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(638, 7)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_done = df[df.partner_name!=\"error\"]\n",
    "df_error = df[df.partner_name==\"error\"]\n",
    "\n",
    "df_done.to_excel(r\"..\\Desktop\\companies_well_scraped.xlsx\", header=True, index=False)\n",
    "df_error.to_excel(r\"..\\Desktop\\companies_error.xlsx\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
